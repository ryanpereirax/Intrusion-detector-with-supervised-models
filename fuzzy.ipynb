{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanpereirax/Intrusion-detector-with-supervised-models/blob/main/fuzzy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmXYxE27601T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "GjRPJtkx0JwD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fuzzy_df = pd.read_csv('/content/Fuzzy_dataset.csv')\n"
      ],
      "metadata": {
        "id": "LrPgYG8MzBub"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Amostras de 1% do Dataset Original\n",
        "amostras_R = fuzzy_df[fuzzy_df['R'] == 'R'].sample(n=5_000, random_state=42)\n",
        "amostras_T = fuzzy_df[fuzzy_df['R'] == 'T'].sample(n=4_000, random_state=42)\n",
        "fuzzy_df_balanceado = pd.concat([amostras_R, amostras_T]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "fuzzy_df_balanceado.to_csv('fuzzy_df_proporcional_ajustado.csv', index=False)"
      ],
      "metadata": {
        "id": "qfEawR5yzEKx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVjFKk3Lwq7K",
        "outputId": "e7ee7443-90b8-4d75-99d8-851f02135958"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QFiFcb8wwpz",
        "outputId": "43c6b9d0-906c-4adc-934c-e0bec2dec7cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kvnxvFxsaLZL",
        "outputId": "4f57e838-edf1-4b1c-b9a2-eadb674d3fd4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/dos_df_proporcional_ajustado.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f46d58e25722>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Carregar o arquivo para examinar os dados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/dos_df_proporcional_ajustado.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Verificar as primeiras linhas do dataset para compreender sua estrutura\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dos_df_proporcional_ajustado.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregar o arquivo para examinar os dados\n",
        "file_path = \"/content/fuzzy_df_proporcional_ajustado.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Verificar as primeiras linhas do dataset para compreender sua estrutura\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Renomear as colunas para melhor compreensão\n",
        "data.columns = ['Timestamp', 'CAN_ID', 'DLC', 'DATA_0', 'DATA_1', 'DATA_2',\n",
        "                'DATA_3', 'DATA_4', 'DATA_5', 'DATA_6', 'DATA_7', 'Flag']\n",
        "\n",
        "# Verificar se existem valores ausentes\n",
        "missing_data = data.isnull().sum()\n",
        "\n",
        "# Analisar a proporção de flags 'T' e 'R'\n",
        "flag_distribution = data['Flag'].value_counts(normalize=True)\n",
        "\n",
        "# Examinar estatísticas descritivas para colunas numéricas\n",
        "numeric_stats = data.describe()\n",
        "\n",
        "missing_data, flag_distribution, numeric_stats\n"
      ],
      "metadata": {
        "id": "Y3FqQEfybDu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter o Timestamp para um formato de data legível para análise temporal\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], unit='s')\n",
        "\n",
        "# Criar uma nova coluna para armazenar apenas a hora das mensagens\n",
        "data['Hour'] = data['Timestamp'].dt.hour\n",
        "\n",
        "# Contar o número de mensagens por hora\n",
        "messages_per_hour = data.groupby('Hour').size()\n",
        "\n",
        "# Identificar quais CAN_IDs são mais frequentes\n",
        "can_id_distribution = data['CAN_ID'].value_counts().head(10)\n",
        "\n",
        "messages_per_hour, can_id_distribution\n"
      ],
      "metadata": {
        "id": "-0bTd8Q3bFuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar a distribuição dos CAN_IDs em relação às flags 'T' e 'R'\n",
        "can_id_flag_distribution = data.groupby(['CAN_ID', 'Flag']).size().unstack(fill_value=0)\n",
        "\n",
        "# Explorar padrões nos campos 'DATA_X' para mensagens com flag 'T'\n",
        "data_injected = data[data['Flag'] == 'T']\n",
        "data_normal = data[data['Flag'] == 'R']\n",
        "\n",
        "# Calcular estatísticas descritivas para os campos 'DATA_X' para cada tipo de flag\n",
        "data_injected_stats = data_injected[['DATA_0', 'DATA_1', 'DATA_2', 'DATA_3',\n",
        "                                     'DATA_4', 'DATA_5', 'DATA_6', 'DATA_7']].describe()\n",
        "\n",
        "data_normal_stats = data_normal[['DATA_0', 'DATA_1', 'DATA_2', 'DATA_3',\n",
        "                                 'DATA_4', 'DATA_5', 'DATA_6', 'DATA_7']].describe()\n",
        "\n",
        "can_id_flag_distribution, data_injected_stats, data_normal_stats\n"
      ],
      "metadata": {
        "id": "jIg3F0ckbH5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar a distribuição temporal das mensagens com CAN_ID '0000' (flag 'T')\n",
        "injected_id_0000 = data[(data['CAN_ID'] == '0000') & (data['Flag'] == 'T')]\n",
        "\n",
        "# Contagem de mensagens injetadas por minuto\n",
        "injected_id_0000['Minute'] = injected_id_0000['Timestamp'].dt.minute\n",
        "injected_per_minute = injected_id_0000.groupby('Minute').size()\n",
        "\n",
        "# Explorar CAN_IDs que aparecem menos frequentemente para identificar possíveis outliers\n",
        "rare_can_ids = data['CAN_ID'].value_counts().tail(10)\n",
        "\n",
        "# Verificar se existem correlações entre os campos 'DATA_X' para as mensagens normais\n",
        "data_normal_numeric = data_normal[['DATA_0', 'DATA_1', 'DATA_2', 'DATA_3',\n",
        "                                   'DATA_4', 'DATA_5', 'DATA_6', 'DATA_7']].apply(lambda x: x.apply(int, base=16))\n",
        "correlation_matrix = data_normal_numeric.corr()\n",
        "\n",
        "injected_per_minute, rare_can_ids, correlation_matrix\n"
      ],
      "metadata": {
        "id": "TujNMr8mbKVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter os campos 'DATA_X' para valores numéricos para análise de correlação\n",
        "data_numeric = data[['DATA_0', 'DATA_1', 'DATA_2', 'DATA_3',\n",
        "                     'DATA_4', 'DATA_5', 'DATA_6', 'DATA_7']].apply(lambda x: x.apply(int, base=16))\n",
        "\n",
        "# Adicionar a flag como numérica (T = 1, R = 0) para facilitar a análise\n",
        "data_numeric['Flag'] = data['Flag'].apply(lambda x: 1 if x == 'T' else 0)\n",
        "\n",
        "# Calcular a correlação entre os campos de dados e a flag\n",
        "correlation_with_flag = data_numeric.corr()['Flag'].drop('Flag')\n",
        "\n",
        "correlation_with_flag\n"
      ],
      "metadata": {
        "id": "uvkaERJkbOoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, roc_auc_score, f1_score\n",
        ")\n",
        "\n",
        "# Supondo que 'data_numeric' esteja carregado corretamente\n",
        "X = data_numeric.drop('Flag', axis=1)\n",
        "y = data_numeric['Flag']\n",
        "\n",
        "# Dividindo os dados em treino (60%), validação (20%) e teste (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicando o MinMaxScaler nos conjuntos de treino, validação e teste\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
        "X_val_scaled = scaler_minmax.transform(X_val)\n",
        "X_test_scaled = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Criando o modelo de Regressão Logística\n",
        "model_minmax = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    solver='lbfgs',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Treinando o modelo com os dados de treino\n",
        "model_minmax.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Função para calcular e exibir métricas\n",
        "def print_metrics(y_true, y_pred, y_prob, dataset_name):\n",
        "    print(f\"\\nMétricas para o conjunto de {dataset_name}:\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_true, y_prob):.4f}\")\n",
        "    print(\"\\nMatriz de Confusão:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\n",
        "# Avaliando no conjunto de treino\n",
        "y_train_pred = model_minmax.predict(X_train_scaled)\n",
        "y_train_prob = model_minmax.predict_proba(X_train_scaled)[:, 1]\n",
        "print_metrics(y_train, y_train_pred, y_train_prob, \"Treino\")\n",
        "\n",
        "\n",
        "# Avaliando no conjunto de teste\n",
        "y_test_pred = model_minmax.predict(X_test_scaled)\n",
        "y_test_prob = model_minmax.predict_proba(X_test_scaled)[:, 1]\n",
        "print_metrics(y_test, y_test_pred, y_test_prob, \"Teste\")\n",
        "\n",
        "# Avaliando no conjunto de validação\n",
        "y_val_pred = model_minmax.predict(X_val_scaled)\n",
        "y_val_prob = model_minmax.predict_proba(X_val_scaled)[:, 1]\n",
        "print_metrics(y_val, y_val_pred, y_val_prob, \"Validação\")\n"
      ],
      "metadata": {
        "id": "fmfDTUTpbR0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, roc_auc_score, f1_score\n",
        ")\n",
        "\n",
        "# Supondo que 'data_numeric' esteja carregado corretamente\n",
        "X = data_numeric.drop('Flag', axis=1)\n",
        "y = data_numeric['Flag']\n",
        "\n",
        "# Dividindo os dados em treino (60%), validação (20%) e teste (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicando o MinMaxScaler nos conjuntos de treino, validação e teste\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
        "X_val_scaled = scaler_minmax.transform(X_val)\n",
        "X_test_scaled = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Criando e treinando o modelo Random Forest com regularização\n",
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Função para calcular e exibir métricas\n",
        "def print_metrics_rf(y_true, y_pred, y_prob, dataset_name):\n",
        "    print(f\"\\nMétricas para o conjunto de {dataset_name}:\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_true, y_prob):.4f}\")\n",
        "    print(\"\\nMatriz de Confusão:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de treino\n",
        "y_train_pred_rf = model_rf.predict(X_train_scaled)\n",
        "y_train_prob_rf = model_rf.predict_proba(X_train_scaled)[:, 1]\n",
        "print_metrics_rf(y_train, y_train_pred_rf, y_train_prob_rf, \"Treino\")\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de teste\n",
        "y_test_pred_rf = model_rf.predict(X_test_scaled)\n",
        "y_test_prob_rf = model_rf.predict_proba(X_test_scaled)[:, 1]\n",
        "print_metrics_rf(y_test, y_test_pred_rf, y_test_prob_rf, \"Teste\")\n",
        "\n",
        "# Avaliar no conjunto de validação\n",
        "y_val_pred_rf = model_rf.predict(X_val_scaled)\n",
        "y_val_prob_rf = model_rf.predict_proba(X_val_scaled)[:, 1]\n",
        "print_metrics_rf(y_val, y_val_pred_rf, y_val_prob_rf, \"Validação\")"
      ],
      "metadata": {
        "id": "IoaJ_B9_fIJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, roc_auc_score, f1_score\n",
        ")\n",
        "\n",
        "# Supondo que 'data_numeric' esteja carregado corretamente\n",
        "X = data_numeric.drop('Flag', axis=1)\n",
        "y = data_numeric['Flag']\n",
        "\n",
        "# Dividindo os dados em treino (60%), validação (20%) e teste (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicando o MinMaxScaler nos conjuntos de treino, validação e teste\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
        "X_val_scaled = scaler_minmax.transform(X_val)\n",
        "X_test_scaled = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Criando e treinando o modelo de Árvore de Decisão com regularização\n",
        "model_tree = DecisionTreeClassifier(\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_tree.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Função para calcular e exibir métricas\n",
        "def print_metrics_tree(y_true, y_pred, y_prob, dataset_name):\n",
        "    print(f\"\\nMétricas para o conjunto de {dataset_name}:\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_true, y_prob):.4f}\")\n",
        "    print(\"\\nMatriz de Confusão:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de treino\n",
        "y_train_pred_tree = model_tree.predict(X_train_scaled)\n",
        "y_train_prob_tree = model_tree.predict_proba(X_train_scaled)[:, 1]\n",
        "print_metrics_tree(y_train, y_train_pred_tree, y_train_prob_tree, \"Treino\")\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de teste\n",
        "y_test_pred_tree = model_tree.predict(X_test_scaled)\n",
        "y_test_prob_tree = model_tree.predict_proba(X_test_scaled)[:, 1]\n",
        "print_metrics_tree(y_test, y_test_pred_tree, y_test_prob_tree, \"Teste\")\n",
        "\n",
        "# Avaliar no conjunto de validação\n",
        "y_val_pred_tree = model_tree.predict(X_val_scaled)\n",
        "y_val_prob_tree = model_tree.predict_proba(X_val_scaled)[:, 1]\n",
        "print_metrics_tree(y_val, y_val_pred_tree, y_val_prob_tree, \"Validação\")"
      ],
      "metadata": {
        "id": "cYiVcoKUhBA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, roc_auc_score, f1_score\n",
        ")\n",
        "\n",
        "# Supondo que 'data_numeric' esteja carregado corretamente\n",
        "X = data_numeric.drop('Flag', axis=1)\n",
        "y = data_numeric['Flag']\n",
        "\n",
        "# Dividindo os dados em treino (60%), validação (20%) e teste (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicando o MinMaxScaler nos conjuntos de treino, validação e teste\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
        "X_val_scaled = scaler_minmax.transform(X_val)\n",
        "X_test_scaled = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Criando e treinando o modelo de Gradient Boosting\n",
        "model_gb = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_gb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Função para calcular e exibir métricas\n",
        "def print_metrics_gb(y_true, y_pred, y_prob, dataset_name):\n",
        "    print(f\"\\nMétricas para o conjunto de {dataset_name}:\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_true, y_prob):.4f}\")\n",
        "    print(\"\\nMatriz de Confusão:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# Avaliar no conjunto de treino\n",
        "y_train_pred_gb = model_gb.predict(X_train_scaled)\n",
        "y_train_prob_gb = model_gb.predict_proba(X_train_scaled)[:, 1]\n",
        "print_metrics_gb(y_train, y_train_pred_gb, y_train_prob_gb, \"Treino\")\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de teste\n",
        "y_test_pred_gb = model_gb.predict(X_test_scaled)\n",
        "y_test_prob_gb = model_gb.predict_proba(X_test_scaled)[:, 1]\n",
        "print_metrics_gb(y_test, y_test_pred_gb, y_test_prob_gb, \"Teste\")\n",
        "\n",
        "# Avaliar no conjunto de validação\n",
        "y_val_pred_gb = model_gb.predict(X_val_scaled)\n",
        "y_val_prob_gb = model_gb.predict_proba(X_val_scaled)[:, 1]\n",
        "print_metrics_gb(y_val, y_val_pred_gb, y_val_prob_gb, \"Validação\")"
      ],
      "metadata": {
        "id": "NvBtNbmBiJ-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, roc_auc_score, f1_score\n",
        ")\n",
        "\n",
        "# Supondo que 'data_numeric' esteja carregado corretamente\n",
        "X = data_numeric.drop('Flag', axis=1)\n",
        "y = data_numeric['Flag']\n",
        "\n",
        "# Dividindo os dados em treino (60%), validação (20%) e teste (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicando o MinMaxScaler nos conjuntos de treino, validação e teste\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
        "X_val_scaled = scaler_minmax.transform(X_val)\n",
        "X_test_scaled = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Criando e treinando o modelo K-Nearest Neighbors\n",
        "model_knn = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    weights='uniform',\n",
        "    metric='minkowski',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model_knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Função para calcular e exibir métricas\n",
        "def print_metrics_knn(y_true, y_pred, y_prob, dataset_name):\n",
        "    print(f\"\\nMétricas para o conjunto de {dataset_name}:\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_true, y_prob):.4f}\")\n",
        "    print(\"\\nMatriz de Confusão:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de treino\n",
        "y_train_pred_knn = model_knn.predict(X_train_scaled)\n",
        "y_train_prob_knn = model_knn.predict_proba(X_train_scaled)[:, 1]\n",
        "print_metrics_knn(y_train, y_train_pred_knn, y_train_prob_knn, \"Treino\")\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de teste\n",
        "y_test_pred_knn = model_knn.predict(X_test_scaled)\n",
        "y_test_prob_knn = model_knn.predict_proba(X_test_scaled)[:, 1]\n",
        "print_metrics_knn(y_test, y_test_pred_knn, y_test_prob_knn, \"Teste\")\n",
        "\n",
        "# Avaliar no conjunto de validação\n",
        "y_val_pred_knn = model_knn.predict(X_val_scaled)\n",
        "y_val_prob_knn = model_knn.predict_proba(X_val_scaled)[:, 1]\n",
        "print_metrics_knn(y_val, y_val_pred_knn, y_val_prob_knn, \"Validação\")\n"
      ],
      "metadata": {
        "id": "x5UYQDmmjRQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, roc_auc_score, f1_score\n",
        ")\n",
        "\n",
        "# Supondo que 'data_numeric' esteja carregado corretamente\n",
        "X = data_numeric.drop('Flag', axis=1)\n",
        "y = data_numeric['Flag']\n",
        "\n",
        "# Dividindo os dados em treino (60%), validação (20%) e teste (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicando o MinMaxScaler nos conjuntos de treino, validação e teste\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
        "X_val_scaled = scaler_minmax.transform(X_val)\n",
        "X_test_scaled = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Criando e treinando o modelo SVM\n",
        "model_svm = SVC(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Função para calcular e exibir métricas\n",
        "def print_metrics_svm(y_true, y_pred, y_prob, dataset_name):\n",
        "    print(f\"\\nMétricas para o conjunto de {dataset_name}:\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_true, y_prob):.4f}\")\n",
        "    print(\"\\nMatriz de Confusão:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# Avaliar no conjunto de treino\n",
        "y_train_pred_svm = model_svm.predict(X_train_scaled)\n",
        "y_train_prob_svm = model_svm.predict_proba(X_train_scaled)[:, 1]\n",
        "print_metrics_svm(y_train, y_train_pred_svm, y_train_prob_svm, \"Treino\")\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de teste\n",
        "y_test_pred_svm = model_svm.predict(X_test_scaled)\n",
        "y_test_prob_svm = model_svm.predict_proba(X_test_scaled)[:, 1]\n",
        "print_metrics_svm(y_test, y_test_pred_svm, y_test_prob_svm, \"Teste\")\n",
        "\n",
        "# Avaliar no conjunto de validação\n",
        "y_val_pred_svm = model_svm.predict(X_val_scaled)\n",
        "y_val_prob_svm = model_svm.predict_proba(X_val_scaled)[:, 1]\n",
        "print_metrics_svm(y_val, y_val_pred_svm, y_val_prob_svm, \"Validação\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JwUOkVFDmFkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, roc_auc_score, f1_score\n",
        ")\n",
        "\n",
        "X = data_numeric.drop('Flag', axis=1)\n",
        "y = data_numeric['Flag']\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
        "X_val_scaled = scaler_minmax.transform(X_val)\n",
        "X_test_scaled = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Criando e treinando o modelo XGBoost\n",
        "model_xgb = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=10,\n",
        "    min_child_weight=1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Função para calcular e exibir métricas\n",
        "def print_metrics_xgb(y_true, y_pred, y_prob, dataset_name):\n",
        "    print(f\"\\nMétricas para o conjunto de {dataset_name}:\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_true, y_prob):.4f}\")\n",
        "    print(\"\\nMatriz de Confusão:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de treino\n",
        "y_train_pred_xgb = model_xgb.predict(X_train_scaled)\n",
        "y_train_prob_xgb = model_xgb.predict_proba(X_train_scaled)[:, 1]\n",
        "print_metrics_xgb(y_train, y_train_pred_xgb, y_train_prob_xgb, \"Treino\")\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de teste\n",
        "y_test_pred_xgb = model_xgb.predict(X_test_scaled)\n",
        "y_test_prob_xgb = model_xgb.predict_proba(X_test_scaled)[:, 1]\n",
        "print_metrics_xgb(y_test, y_test_pred_xgb, y_test_prob_xgb, \"Teste\")\n",
        "\n",
        "# Avaliar no conjunto de validação\n",
        "y_val_pred_xgb = model_xgb.predict(X_val_scaled)\n",
        "y_val_prob_xgb = model_xgb.predict_proba(X_val_scaled)[:, 1]\n",
        "print_metrics_xgb(y_val, y_val_pred_xgb, y_val_prob_xgb, \"Validação\")\n"
      ],
      "metadata": {
        "id": "JNokTQ1DowJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, roc_auc_score, f1_score\n",
        ")\n",
        "\n",
        "# Supondo que 'data_numeric' esteja carregado corretamente\n",
        "X = data_numeric.drop('Flag', axis=1)\n",
        "y = data_numeric['Flag']\n",
        "\n",
        "# Dividindo os dados em treino (60%), validação (20%) e teste (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Aplicando o MinMaxScaler nos conjuntos de treino, validação e teste\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
        "X_val_scaled = scaler_minmax.transform(X_val)\n",
        "X_test_scaled = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Criando e treinando o modelo CatBoost\n",
        "model_cb = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=10,\n",
        "    random_seed=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "model_cb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Função para calcular e exibir métricas\n",
        "def print_metrics_cb(y_true, y_pred, y_prob, dataset_name):\n",
        "    print(f\"\\nMétricas para o conjunto de {dataset_name}:\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, pos_label=1):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_true, y_prob):.4f}\")\n",
        "    print(\"\\nMatriz de Confusão:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de treino\n",
        "y_train_pred_cb = model_cb.predict(X_train_scaled)\n",
        "y_train_prob_cb = model_cb.predict_proba(X_train_scaled)[:, 1]\n",
        "print_metrics_cb(y_train, y_train_pred_cb, y_train_prob_cb, \"Treino\")\n",
        "\n",
        "\n",
        "# Avaliar no conjunto de teste\n",
        "y_test_pred_cb = model_cb.predict(X_test_scaled)\n",
        "y_test_prob_cb = model_cb.predict_proba(X_test_scaled)[:, 1]\n",
        "print_metrics_cb(y_test, y_test_pred_cb, y_test_prob_cb, \"Teste\")\n",
        "\n",
        "# Avaliar no conjunto de validação\n",
        "y_val_pred_cb = model_cb.predict(X_val_scaled)\n",
        "y_val_prob_cb = model_cb.predict_proba(X_val_scaled)[:, 1]\n",
        "print_metrics_cb(y_val, y_val_pred_cb, y_val_prob_cb, \"Validação\")\n"
      ],
      "metadata": {
        "id": "D1bcOURixBnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}